# -*- coding: utf-8 -*-
"""AgriNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1seECM-ft69jUnixxK5D4iOy8WbMCoRqS
"""

!cp -a /kaggle/input/bangladeshi-crops-disease-dataset/CropDisease/Crop___DIsease/. /kaggle/working/Dataset
!rm -r /kaggle/working/Dataset/Invalid

!mv '/kaggle/working/Dataset/Rice___Brown_Spot' '/kaggle/working/Dataset/Rice: Brown Spot'
!mv '/kaggle/working/Dataset/Corn___Gray_Leaf_Spot' '/kaggle/working/Dataset/Corn: Gray Leaf Spot'
!mv '/kaggle/working/Dataset/Rice___Leaf_Blast' '/kaggle/working/Dataset/Rice: Leaf Blast'
!mv '/kaggle/working/Dataset/Corn___Leaf_Blight' '/kaggle/working/Dataset/Corn: Leaf Blight'
!mv '/kaggle/working/Dataset/Potato___Early_Blight' '/kaggle/working/Dataset/Potato: Early Blight'
!mv '/kaggle/working/Dataset/Potato___Healthy' '/kaggle/working/Dataset/Potato: Healthy'
!mv '/kaggle/working/Dataset/Rice___Healthy' '/kaggle/working/Dataset/Rice: Healthy'
!mv '/kaggle/working/Dataset/Wheat___Brown_Rust' '/kaggle/working/Dataset/Wheat: Brown Rust'
!mv '/kaggle/working/Dataset/Potato___Late_Blight' '/kaggle/working/Dataset/Potato: Late Blight'
!mv '/kaggle/working/Dataset/Wheat___Healthy' '/kaggle/working/Dataset/Wheat: Healthy'
!mv '/kaggle/working/Dataset/Wheat___Yellow_Rust' '/kaggle/working/Dataset/Wheat: Yellow Rust'
!mv '/kaggle/working/Dataset/Corn___Healthy' '/kaggle/working/Dataset/Corn: Healthy'
!mv '/kaggle/working/Dataset/Corn___Common_Rust' '/kaggle/working/Dataset/Corn: Common Rust'
!mv '/kaggle/working/Dataset/Rice___Hispa' '/kaggle/working/Dataset/Rice: Hispa'

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import numpy as np
import os

# Configuration
DATASET_DIRECTORY = '/kaggle/working/Dataset/'
BATCH_SIZE = 32
IMAGE_SIZE = 256
VALIDATION_SPLIT = 0.2
LEARNING_RATE = 1e-4
EPOCHS = 32
CLASS_NAMES = []

# Load class names
for directory in sorted(os.listdir(DATASET_DIRECTORY)):
    CLASS_NAMES.append(directory)
NUM_CLASSES = len(CLASS_NAMES)

# Preprocessing function
def preprocess(image, label):
    image = tf.cast(image / 255.0, tf.float32)
    return image, label

# Load dataset
train_ds = tf.keras.utils.image_dataset_from_directory(
    directory=DATASET_DIRECTORY,
    label_mode='categorical',
    class_names=CLASS_NAMES,
    batch_size=BATCH_SIZE,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    validation_split=VALIDATION_SPLIT,
    subset='training',
    seed=123
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    directory=DATASET_DIRECTORY,
    label_mode='categorical',
    class_names=CLASS_NAMES,
    batch_size=BATCH_SIZE,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    validation_split=VALIDATION_SPLIT,
    subset='validation',
    seed=123
)

# Optimize dataset for performance
train_ds = train_ds.cache().prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.cache().prefetch(tf.data.AUTOTUNE)

# Visualize some training data
for images, labels in train_ds.take(1):
    plt.figure(figsize=(8, 8))
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        plt.imshow(images[i] / 255.0)
        plt.title(CLASS_NAMES[np.argmax(labels[i])])
        plt.axis('off')
    plt.show()

# Build hybrid model using EfficientNetB0 as a base
base_model = EfficientNetB0(include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), weights='imagenet')

# Unfreeze some layers of EfficientNetB0 for fine-tuning
base_model.trainable = True  # Unfreeze the base model

# Optionally, freeze the bottom layers to retain general features, but unfreeze the top layers
for layer in base_model.layers[:-10]:  # Unfreeze the last 10 layers (adjust as needed)
    layer.trainable = False

# Build the model
model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(name='global_avg_pooling'),
    layers.Dense(128, activation='relu', name='dense_1'),
    layers.Dropout(0.5, name='dropout'),
    layers.Dense(NUM_CLASSES, activation='softmax', name='output_layer')
])

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=['accuracy']
)

# Print model summary
model.summary()

# Set up model checkpoint callback
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='/kaggle/working/checkpoint/',
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True
)

# Train the model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[model_checkpoint_callback]
)

# Evaluate and calculate metrics
val_predictions = model.predict(val_ds)
val_labels = np.concatenate([labels.numpy() for _, labels in val_ds])
val_predictions_classes = np.argmax(val_predictions, axis=1)
val_true_classes = np.argmax(val_labels, axis=1)

# Generate classification report
report = classification_report(val_true_classes, val_predictions_classes, target_names=CLASS_NAMES, output_dict=True)

# Extract metrics
accuracy = report['accuracy']
precision = report['weighted avg']['precision']
recall = report['weighted avg']['recall']
f1_score = report['weighted avg']['f1-score']

# Display metrics
print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1_score:.2f}")